{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels(RGB)\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#CONSTANT\n",
    "BATCH_SIZE = 128 # images count after each neural network calculate loss error\n",
    "NB_EPOCH = 20 # count of epoch\n",
    "NB_CLASSES = 10 # count of images classes types\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2 # 20% for test of images and 80% for training \n",
    "OPTIM = RMSprop() \n",
    "\n",
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,676,842\n",
      "Trainable params: 1,676,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    }
   ],
   "source": [
    "#convert to categorical\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "#normalization images to 0-1 range from 0-255\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "#network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3), padding=\"same\", \n",
    "                 input_shape = (IMG_ROWS, IMG_COLS, IMG_CHANNELS))) # create feature map(contrast image)\n",
    "model.add(Activation(\"relu\")) # save active pixels >0\n",
    "model.add(Conv2D(32,(3,3), padding = \"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2))) # divide image to square 2*2 and get max value\n",
    "model.add(Dropout(0.25)) # 25% filters off \n",
    "model.add(Conv2D(64,(3,3),padding = 'same'))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten()) # create vector\n",
    "model.add(Dense(512)) # vector of 512 elements\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(NB_CLASSES)) # create finish vector with size 10 \n",
    "model.add(Activation('softmax')) # find max activation element(find type of object)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 72s 2ms/step - loss: 1.7489 - accuracy: 0.3679 - val_loss: 1.4381 - val_accuracy: 0.4826\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 1.2581 - accuracy: 0.5556 - val_loss: 1.1256 - val_accuracy: 0.6105\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 1.0322 - accuracy: 0.6343 - val_loss: 0.9675 - val_accuracy: 0.6642\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.8793 - accuracy: 0.6912 - val_loss: 0.8577 - val_accuracy: 0.7000\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.7542 - accuracy: 0.7361 - val_loss: 0.8426 - val_accuracy: 0.7078\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.6441 - accuracy: 0.7721 - val_loss: 0.7277 - val_accuracy: 0.7504\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.5551 - accuracy: 0.8087 - val_loss: 0.8447 - val_accuracy: 0.7270\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.4692 - accuracy: 0.8356 - val_loss: 0.7820 - val_accuracy: 0.7515\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.3946 - accuracy: 0.8626 - val_loss: 0.7547 - val_accuracy: 0.7682\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.3229 - accuracy: 0.8867 - val_loss: 0.8203 - val_accuracy: 0.7623\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.2741 - accuracy: 0.9044 - val_loss: 0.9120 - val_accuracy: 0.7550\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.2376 - accuracy: 0.9171 - val_loss: 0.9498 - val_accuracy: 0.7435\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.2047 - accuracy: 0.9283 - val_loss: 0.9467 - val_accuracy: 0.7614\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.1819 - accuracy: 0.9372 - val_loss: 0.9962 - val_accuracy: 0.7636\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.1596 - accuracy: 0.9456 - val_loss: 1.0209 - val_accuracy: 0.7675\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.1437 - accuracy: 0.9512 - val_loss: 1.1256 - val_accuracy: 0.7642\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.1424 - accuracy: 0.9529 - val_loss: 1.1764 - val_accuracy: 0.7569\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 64s 2ms/step - loss: 0.1285 - accuracy: 0.9560 - val_loss: 1.1352 - val_accuracy: 0.7658\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 0.1152 - accuracy: 0.9606 - val_loss: 1.3825 - val_accuracy: 0.7569\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.1045 - accuracy: 0.9643 - val_loss: 1.3919 - val_accuracy: 0.7636\n",
      "10000/10000 [==============================] - 4s 445us/step\n",
      "Test score: 1.452359327697754\n",
      "Test accuracy: 0.7501000165939331\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "model.compile(loss='categorical_crossentropy',optimizer = OPTIM,\n",
    "             metrics = ['accuracy']) # use categorical_crossentropy function to minimase the error \n",
    "model.fit(X_train,Y_train,batch_size=BATCH_SIZE,verbose = VERBOSE, \n",
    "          epochs = NB_EPOCH, validation_split = VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test,Y_test,batch_size=BATCH_SIZE,verbose = VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "#save\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json','w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model_sample.py\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_image(img_path, show=False):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(32, 32))\n",
    "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img_tensor[0])                           \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "with open('cifar10_architecture.json','r') as f:\n",
    "    model_json = f.read()\n",
    "\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('cifar10_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = {\n",
    "    0: \"airplane\",\n",
    "    1:\"automobile\",\n",
    "    2:\"bird\",\n",
    "    3:\"cat\",\n",
    "    4:\"deer\",\n",
    "    5:\"dog\",\n",
    "    6:\"frog\",\n",
    "    7:\"horse\",\n",
    "    8:\"ship\",\n",
    "    9:\"truck\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAQcUlEQVR4nO3db6ykZXnH8XueZ/6cOX93l91ld3FXqRaQvyluKTRgWq1EbTXFUJE2xtCKKKkBi5ZXfdHaaNr6ByVWbOMbRcwu2mg0JZqipX+UUK1aBCsVyG4pC7scdvecnXPmzJl5pi/69v79NufJHHphvp+Xz5V75pln5ponua/nvq/GeDxOAOIp/r9PAEAeyQkERXICQZGcQFAkJxBU0wVf/Y6/lFO5RaHzutFoiOOlHqNDqSzNuGRi8jzyx1NKqTCxhvnM7nqMC/Oa8hwn/7/pzjEl9VXrc/fvpce5AsF4XE30PBzzVdtzrKr8OVbVyL2bjHztYzdkg9w5gaBITiAokhMIiuQEgiI5gaBITiAoW0pxJYc6Gg09P21LB2MTm/QMuyulmFhlzqNpTnIsX9NdK3eO9f5vJ126qbueYvLnoU/EL/rY+O+gKHRZr84CE+6cQFAkJxAUyQkERXICQZGcQFAkJxCULaU4fjpfxMxKBbdioqhdHtj4qhS3gsTFmuYcXZlFrYLxJYV6ZRY3rt7rvXDq7nVV9/zd271Q14Q7JxAUyQkERXICQZGcQFAkJxBU7dnaOurPnNWdnawxZjPO0e7ds3GbMVsYZVZW2YzfTt33U69ZZ4zDnRMIiuQEgiI5gaBITiAokhMIiuQEgtqcPYTEA+Iv9HR9rXYMbu8YtxeQa7kgI+4c9Rg3Le9bLky47PSiV6/0MfHyncCdEwiK5ASCIjmBoEhOICiSEwiK5ASCql1KqVuOqPVeNbbGr/tejut6XbnrUaMLuD2PCb+eV7OFRu3XzJ//TOrJMYNmR8bWBvVaJNS5jpN+Pe6cQFAkJxAUyQkERXICQZGcQFAkJxBU/Q2+zCoMOWTibahPU9IRJQc3Zq6r/68uO+8sGfvhE8dkbLFXyVizjL2Cp/7r6djW5rqMffSWN2WPj8qWHPOTJw7L2C0fuFHG9v7yH8pYnQ25Jn0NuXMCQZGcQFAkJxAUyQkERXICQZGcQFD1O1u7zr+izDJ2HZ7H5n9iwlWFkXmr3/m1V+rTqPSHPnfvNhnrrQxk7J5/eVJEJrvC4fTjNl46qEpdIrrz/VfL2LHHdOnjwf/8afb4ow8/Jscc/NIBGXvZ5bfI2Gg0krFJ9z2pgzsnEBTJCQRFcgJBkZxAUCQnENSm7CGkYrbVwYTfK6WULvyF3dnjV1+aP55SSgszUzLW6ei9asqmPo/njuv9b664MP8w/UM//h85Rs+Rnq4x98ZngN31/ev3/aaM3feNb8pYVelZ0r/78lezx48dfUqOWe4flbE9Y321orea4M4JBEVyAkGRnEBQJCcQFMkJBEVyAkFtSmfrye9Ho0sA115xjoytrTyePb5t9hflmKkZfUkK8182NlP2c7NtGTt/z0z2+CP/JYekU+v1Skvjhj7HditfQrrztjfIMW+/6T0y9luve42MHTj4RRlbPfFc9vhKb0mOueBNfypjvkWCDNUaN+nn4blzAkGRnEBQJCcQFMkJBEVyAkGRnEBQ9fcQMvPQrVZ+6/zhcCjH7N5zhoy99vwdMrZvmx73kyfypZS77/mMHPMH77xZxk729eqS7fMLMjaodAmj18u/prtWjYYuzThFf1HGdhb97PFbb/8jOeaqy/fL2MGDB2Vs2DspY9Ot/O/q7Df/sRxjtnaqvRcQewgBkEhOICiSEwiK5ASCIjmBoEhOIKjapZRmUcrYVRftzB5//NAzcszrzsuPSSmlM7fPytj0vP5/uWL/r2aP/8oll8gx777pOhkzFZHU7+uWC5++6wsy1kj5ssiaOJ6Sn+b/6AeukbGFtt5Y66t//w/Z4/ff/6wc8/WvfFnG0rouO5376nfL2HIj/12P7eZk+jTqmnRn66JG9YU7JxAUyQkERXICQZGcQFAkJxAUyQkE1XBTxte+7zMy+BuXvkSOa7byZZZukV+tklJKe3fNydiWUld8WmfoMstgkC8duBUfo0qf47BalrH333qjjFWmpXdHbKzVGOv3Wjy+LmP3ffNbMvaROz4hYw8++Gj2eKtxQo7ZsjAtY1OX6NU9ZVqTsShcXqiYq764DeC+9OHfy/5AuHMCQZGcQFAkJxAUyQkERXICQZGcQFB2Vcrr9++Vsa0zXRmbmcmXI6am9EqL6Wnd7v35uz8nYx/ZsU/G/uK3r84e73by/UlSSmlgyiyDoR73+bu/JmPXXa/bsy8+cyR7fOdOvUqnPzgmYzfccIOM9Xp6hUl/JX+8OaO/l9dff7uMfeeRwzJWFfo6DkVfnNKsCHJG5vZT9zXr2fjSGe6cQFAkJxAUyQkERXICQZGcQFD2wfdvPfCQDDbMHvhT3fwkcFnqfYe6XT0rWE3pabWlQ/oB8c5Lz80e35H0mLHpor3SX5Wx3qqe5T26eErGrrz85dnje3bpmfKLLjpbxopSP4zebOoZw2YzP5P+gx8+LMfcecfHZexdN98mY9fc9jcypmZXi1G9FgibMVtbic2k6jwsnxIPvgMvOiQnEBTJCQRFcgJBkZxAUCQnEJQtpTzwT9+VwU6nI8ep12w29XP2RaH/J9w29/9x6ZUy9rHL3pw9/vGv/5Ucs/eEeAI8pTQa6XYGU9P6Ye733PwuGbvu+rdljx8+9JQc87d3fVLGnj2iW15c/EsXytihJw9lj+/edZYcc+a+3TK241X6M7vyRh2uJFL3vVzpRv0O3O9UlV9SopQCvOiQnEBQJCcQFMkJBEVyAkGRnEBQdg8hV2ZpJN22YGUtv3qja8olhZlqbrX0e53/0D/K2E1vvDZ7/MpKlz2+94kPy9i+D/6JjN37+XtkzK3GOXBvvjv0aEnvE7RnQbeu2Ln3pTLWGfdlbPfuXfn32qXLJVsuu0nGGqa84TpR1+kovRkrT9xZuJLJJHHnBIIiOYGgSE4gKJITCIrkBIIiOYGgbCllfV13UG4kPS2vntofDvUUdMN0lC7NfPgFi/l2Biml1Lg7X6Z4ZNceOWbmR/8sY//27Qdk7PsP/0jGTp3SG3z1V/Kdo6vekhzz8rfeIWNb2nrcK7bqLuCHfnp/9vj8Nt0W4md6T7Pa6pQpbMnPvF7dcSpW9/UU7pxAUCQnEBTJCQRFcgJBkZxAUCQnEJQtpVQmvLSiSyllmc/5wameHHP7bTfKmCvpvPVt18vYKz95IHt89tZ3yDEzR3Rp5s8+e5eMdU/ocsl0V/8HLsxtyx5/759/So75zsNPy1hn+gwZq5q69rHrnF/PHn/kiP7O7NKTmv/7qmrWNr/UvTv0KqPHj+jvxXWbditd7MeeIO6cQFAkJxAUyQkERXICQZGcQFC2HcN93/hXGTy+rGfxZqfzXaqfO6lnzvaduV3GDhzQnZAf/fH3ZawnZlAvXtdTf4/u3CFjC7r5dmq19D5BH/zQp2VsLPZOGg1064fnzXU8tjyQsaXemowtzOY/3GNPH5djfnZcv1ddzXF+BrXT0jOrU2ZvquOr+jP7vYd0cCi6nzeGehrX5dm9H/pd2jEALyYkJxAUyQkERXICQZGcQFAkJxCUffB9bV1PlR9fWpYxVUrZsUXvYeOeJn7nje/Vw8wU9dsve1X2+JHX6m7Yr9l/uYxd85bfl7HVvtknqK8XCUy38x3CS/ew/Fg/6D1q6HFzXb1Pk1pccOHerXLMUyd0F+2hKUU4O8U+R8s9fX1PDPTCCNcxXRerfOmjFKFJPw/PnRMIiuQEgiI5gaBITiAokhMIiuQEgrKllDmxUiGllM6ZOUvGhmJFxVS3Lce47epVe4fT+cL3fpA9/tzz+RYIKaU0MzsvY4O1FRmz2+2bafn1Kv/ZOoUue7Q7+r0WxvnSTEq+w3aq8ufoShH7+/p7+e5jizJmXjKtrOSv8Y65aTlmcTnfST2llNpN/ZmPruhSYUOsPEkppaH4rguxoub/xmy8Yzd3TiAokhMIiuQEgiI5gaBITiAokhMIypZS5me7MuZaJFTtfM5XSU9rl+pR/5RS6cospkyxJs5xfl6vjllbM5/LvJfYp+u0MVXecOUjdz3murr81Wzp81erMNzqjAv26bLT08/qVUs90xH7pAh2dYUozVf6Z9wRrUFSSunoSr11JKplhPt9mJ+3xJ0TCIrkBIIiOYGgSE4gKJITCIrkBIKypZSG2bLI9QZRqzDEwoeUUkpFoV+vMrUIuxpEjTNjXOlgaMpHA/Phptt6hclgkF8ZUUyZa2+uVWlWYZSl/rrVdXTXY85sQnbVxWfK2JPPnJSx0SB//q2Wfi/Xo6Rs6c/cSvp7GY1dHxjzm5sg7pxAUCQnEBTJCQRFcgJBkZxAUHa2ttnU4YGZuWyqvWomvV99Smk41E9Rq3+eyjxU3mrpGTxnsKZn99w+PCrmZqgrO9us38vNvE7aGfN6z5/StIw4sZxvXfHfi/r6ttt6b6pOU3/mfqFjzaFbJLAJP+QM7pxAUCQnEBTJCQRFcgJBkZxAUCQnEJQtpbjp/KLWw+N6jC0dTDjmSkRVpafsmw39UHmnqUsw7uF8tReTKzesmvJR3XKJKukMh7rsVIz19Sga+hrPTesNgdQeTi/Zol+vWeprv2q6Xu/p6vNvTevu4UeW8u0fRqPJlqq4cwJBkZxAUCQnEBTJCQRFcgJBkZxAUPVLKWalxUiMq7sowpUiirEpiyS3D0ye3UPIrEYwu/5b6rMNTMsCdx2HhR7oVtxUYg8kW6oy/+1mwYff96nIx2ZnTKdvs2/ScFmf/7zpkbB9qy73PH8q3327Z1ey0Nka+LlBcgJBkZxAUCQnEBTJCQRFcgJB1S6luJgcM67XBsG91zjpVRNu9Umd9yob+hxHyU2V66n+tb5YNdHQ59E0JRG3MkK1fkhJXyvXwVyVPVJKaWA2UVsf6FinzL9my6w8cb+d6Y7e/Ovss3SHc7/CRN3T9OeqgzsnEBTJCQRFcgJBkZxAUCQnEBTJCQRlaw1uitpNNKtyRN0NkNSKiZROt9JCrY7Rr+dWTNgVK2YVSWroKfa19Xx5w56Heatm0qWDaqRPstnKx9x5uE7frofNYF1fj7bomN4yZRvXO6Y90rHtM/paHV/WZafzduf7wPz74VNyDKtSgJ8jJCcQFMkJBEVyAkGRnEBQJCcQVO1VKb6UsvHXs+9lptHHgzUZU5uQuWntojCbRbkW92bcymq+lXpK+nM3TK+U4VBfq9VKlzdK04K9XeXLCrZUZTY8G9Xc7ErFKrcnmPkxup4zbbNqqSz0dRTVntQYu1Up5gMI3DmBoEhOICiSEwiK5ASCIjmBoOo/+G5iI7F/zGCkZ/fMljl25q8s9SxpY5SfcXN7C43NDOS62RdnuO727tEzf2q2tjKzru5hdBfrmP10ym5+3NjMDPuFAGZmW39l+vzNb6dws65Nc60KPa7T1rHlfGPrNDadz8fVxvcX4s4JBEVyAkGRnEBQJCcQFMkJBEVyAkHZUoqbDq8zbuimw02rBtf4wXXYLsS0vHvIXpWBUvIdpd04937yWpm/zeG6/l6mTHmgNO23+339cL5+PV06sF3RzXmocYV5gL3unlAu5spth5eWxHls/OF2hzsnEBTJCQRFcgJBkZxAUCQnEBTJCQTVqLNNPIDNx50TCIrkBIIiOYGgSE4gKJITCIrkBIL6X6F2k2vupSWbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'airplane'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_image = load_image(\"plane.jpg\", True)\n",
    "pred = model.predict_classes(target_image)\n",
    "target_classes[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
